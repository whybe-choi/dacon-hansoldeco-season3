{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3nICHj773X9Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import yaml\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중국어 문자 감지 함수 (감지 기준 조절 가능)\n",
    "def contains_chinese(text, min_count=1):\n",
    "    chinese_count = 0\n",
    "    for char in text:\n",
    "        if \"\\u4e00\" <= char <= \"\\u9fff\":\n",
    "            chinese_count += 1\n",
    "            if chinese_count >= min_count:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# 특수문자/이모티콘 감지 함수\n",
    "def contains_special_chars(text):\n",
    "    import re\n",
    "\n",
    "    # 일반 문자, 숫자, 기본 문장부호 외의 문자 감지\n",
    "    special_pattern = re.compile(r\"[^\\w\\s,.?!:;()\\-\\'\\\"가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z0-9]\")\n",
    "    return bool(special_pattern.search(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4JGt9te58WF"
   },
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path = \".\"  # your path\n",
    "\n",
    "data_path = f\"{local_path}/data\"\n",
    "\n",
    "test_path = f\"{data_path}/ver-1-test.csv\"  # processed test data\n",
    "prompts_path = f\"{data_path}/prompts.yaml\"  # prompts\n",
    "\n",
    "qe_filename = \"ver-1-preprocessed.jsonl\"  # Query Expansion 결과 저장\n",
    "if os.path.exists(f\"{data_path}/{qe_filename}\"):\n",
    "    raise FileExistsError(f\"'{qe_filename}' already exists.\")\n",
    "\n",
    "temp_qe_filename = \"ver-1.jsonl\"  # Query Expansion 임시 저장\n",
    "if os.path.exists(f\"{local_path}/{temp_qe_filename}\"):\n",
    "    raise FileExistsError(f\"'{temp_qe_filename}' already exists.\")\n",
    "\n",
    "\n",
    "# CSV\n",
    "test = pd.read_csv(test_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "\n",
    "# Prompts\n",
    "with open(prompts_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "system_prompt = prompts[\"query_expansion\"][\"ver_0\"][\"system_prompt\"]\n",
    "user_prompt_template = prompts[\"query_expansion\"][\"ver_1\"][\"user_prompt_template\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion (Ver 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the basic query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"공종2\", \"작업프로세스\", \"사고객체1\", \"사고객체2\", \"인적사고1\", \"사고원인\"]\n",
    "\n",
    "query_list = []\n",
    "for i in range(test.shape[0]):\n",
    "    gongjong = test.loc[i, \"공종2\"]\n",
    "    job_process = test.loc[i, \"작업프로세스\"]\n",
    "    accident_object = test.loc[i, \"사고객체1\"] + \", \" + test.loc[i, \"사고객체2\"]\n",
    "    human_accident = test.loc[i, \"인적사고1\"]\n",
    "    accident_cause = test.loc[i, \"사고원인\"]\n",
    "\n",
    "    user_prompt = user_prompt_template.format(\n",
    "        job_process=job_process,\n",
    "        gongjong=gongjong,\n",
    "        human_accident=human_accident,\n",
    "        accident_object=accident_object,\n",
    "        accident_cause=accident_cause,\n",
    "    )\n",
    "    query_list.append(user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model_id = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "gemma_model_id = \"rtzr/ko-gemma-2-9b-it\"\n",
    "\n",
    "# gemma \n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    gemma_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(gemma_model_id)\n",
    "\n",
    "# qwen \n",
    "qwen_model = AutoModelForCausalLM.from_pretrained(\n",
    "    qwen_model_id,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사용한 프롬프트: \n",
    "\n",
    "    - system 의 경우 `ver 0`\n",
    "    \n",
    "    - user 의 경우 `ver 1`\n",
    "\n",
    "- 기본 쿼리로 사용한 컬럼\n",
    "\n",
    "    - `\"공종2\", \"작업프로세스\", \"사고객체1\", \"사고객체2\", \"인적사고1\", \"사고원인\"`\n",
    "\n",
    "- 1단계: gemma 모델 사용 (메인)\n",
    "\n",
    "    - 최대 5회 재시도 (빈 값일 경우)\n",
    "\n",
    "- 2단계: qwen 모델 사용 (gemma 실패시)\n",
    "\n",
    "    - 최대 10회 재시도 (중국어, 이모티콘, 빈 값일 경우)\n",
    "\n",
    "- 3단계: 다시 gemma 모델 사용 (qwen 실패시)\n",
    "\n",
    "    - 최대 3회 재시도\n",
    "\n",
    "- 마지막 단계\n",
    "\n",
    "    - 빈 값 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qe_path = f\"{local_path}/{temp_qe_filename}\"\n",
    "\n",
    "# 메인 모델, 서브 모델 설정\n",
    "main_model = gemma_model\n",
    "main_tokenizer = gemma_tokenizer\n",
    "sub_model = qwen_model\n",
    "sub_tokenizer = qwen_tokenizer\n",
    "\n",
    "for i in range(len(query_list)):\n",
    "    user_prompt = query_list[i]\n",
    "\n",
    "    # 1단계: gemma 모델 사용 (메인)\n",
    "    print(f\"[{i+1}/{len(query_list)}] Level 1: Gemma\", \"-----\" * 10)\n",
    "    retry_count = 0\n",
    "    max_retries_main = 5\n",
    "\n",
    "    # 최대 5회 재시도 (빈 값일 경우)\n",
    "    while retry_count < max_retries_main:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "\n",
    "        text = main_tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = main_tokenizer([text], return_tensors=\"pt\").to(main_model.device)\n",
    "\n",
    "        generated_ids = main_model.generate(\n",
    "            **model_inputs,\n",
    "            max_new_tokens=1024,\n",
    "            do_sample=True,\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids) :]\n",
    "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "\n",
    "        response = main_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[\n",
    "            0\n",
    "        ]\n",
    "        response = response.strip()\n",
    "\n",
    "        # 빈 값 확인 (gemma는 빈 값만 확인)\n",
    "        if not response:\n",
    "            retry_count += 1\n",
    "            print(\n",
    "                f\"[{i+1}/{len(query_list)}] Empty Response. Retrying... ({retry_count}/{max_retries_main})\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # 유효한 응답을 얻었으면 저장하고 종료\n",
    "        response_split = response.strip().split(\"\\n\")\n",
    "        five_q = {\"questions\": response_split, \"test_id\": test.loc[i, \"ID\"]}\n",
    "        with open(temp_qe_path, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "            f.write(json.dumps(five_q, ensure_ascii=False) + \"\\n\")\n",
    "        print(response)\n",
    "        break\n",
    "\n",
    "    # 2단계: qwen 모델 사용 (gemma 실패시)\n",
    "    if retry_count >= max_retries_main:\n",
    "        print(f\"[{i+1}/{len(query_list)}] Level 2: Qwen\", \"-----\" * 10)\n",
    "        retry_count = 0\n",
    "        max_retries_sub = 10\n",
    "\n",
    "        # 최대 10회 재시도 (중국어, 이모티콘, 빈 값일 경우)\n",
    "        while retry_count < max_retries_sub:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "\n",
    "            text = sub_tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            model_inputs = sub_tokenizer([text], return_tensors=\"pt\").to(\n",
    "                sub_model.device\n",
    "            )\n",
    "\n",
    "            generated_ids = sub_model.generate(\n",
    "                **model_inputs,\n",
    "                max_new_tokens=1024,\n",
    "                do_sample=True,\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            generated_ids = [\n",
    "                output_ids[len(input_ids) :]\n",
    "                for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "\n",
    "            response = sub_tokenizer.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )[0]\n",
    "            response = response.strip()\n",
    "\n",
    "            # qwen은 중국어와 특수문자 모두 확인\n",
    "            if contains_chinese(response) or contains_special_chars(response):\n",
    "                retry_count += 1\n",
    "                print(\n",
    "                    f\"[{i+1}/{len(query_list)}] Chinese/Special Character in Response. Retrying... ({retry_count}/{max_retries_sub})\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # 빈 응답인 경우도 재시도\n",
    "            if not response:\n",
    "                retry_count += 1\n",
    "                print(\n",
    "                    f\"[{i+1}/{len(query_list)}] Empty Response. Retrying... ({retry_count}/{max_retries_sub})\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # 유효한 응답을 얻었으면 저장하고 종료\n",
    "            response_split = response.strip().split(\"\\n\")\n",
    "            five_q = {\"questions\": response_split, \"test_id\": test.loc[i, \"ID\"]}\n",
    "            with open(temp_qe_path, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "                f.write(json.dumps(five_q, ensure_ascii=False) + \"\\n\")\n",
    "            print(response)\n",
    "            break\n",
    "\n",
    "        # 3단계: 다시 gemma 모델 사용 (qwen 실패시)\n",
    "        if retry_count >= max_retries_sub:\n",
    "            print(f\"[{i+1}/{len(query_list)}] Level 3: Gemma\", \"-----\" * 10)\n",
    "            retry_count = 0\n",
    "            max_retries_final = 3\n",
    "\n",
    "            # 최대 3회 재시도\n",
    "            while retry_count < max_retries_final:\n",
    "                messages = [\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt},\n",
    "                ]\n",
    "\n",
    "                text = main_tokenizer.apply_chat_template(\n",
    "                    messages, tokenize=False, add_generation_prompt=True\n",
    "                )\n",
    "                model_inputs = main_tokenizer([text], return_tensors=\"pt\").to(\n",
    "                    main_model.device\n",
    "                )\n",
    "\n",
    "                generated_ids = main_model.generate(\n",
    "                    **model_inputs,\n",
    "                    max_new_tokens=1024,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.1,\n",
    "                )\n",
    "                generated_ids = [\n",
    "                    output_ids[len(input_ids) :]\n",
    "                    for input_ids, output_ids in zip(\n",
    "                        model_inputs.input_ids, generated_ids\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                response = main_tokenizer.batch_decode(\n",
    "                    generated_ids, skip_special_tokens=True\n",
    "                )[0]\n",
    "                response = response.strip()\n",
    "\n",
    "                # 빈 값 확인 (gemma는 빈 값만 확인)\n",
    "                if not response:\n",
    "                    retry_count += 1\n",
    "                    print(\n",
    "                        f\"[{i+1}/{len(query_list)}] Empty Response. Retrying... ({retry_count}/{max_retries_final})\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                # 유효한 응답을 얻었으면 저장하고 종료\n",
    "                response_split = response.strip().split(\"\\n\")\n",
    "                five_q = {\"questions\": response_split, \"test_id\": test.loc[i, \"ID\"]}\n",
    "                with open(temp_qe_path, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "                    f.write(json.dumps(five_q, ensure_ascii=False) + \"\\n\")\n",
    "                print(response)\n",
    "                break\n",
    "\n",
    "            # 모든 단계 실패시 빈 리스트 저장\n",
    "            if retry_count >= max_retries_final:\n",
    "                print(f\"[{i+1}/{len(query_list)}] All Steps Failed. Empty List Saved.\")\n",
    "                five_q = {\"questions\": [], \"test_id\": test.loc[i, \"ID\"]}\n",
    "                with open(temp_qe_path, \"a\", encoding=\"utf-8-sig\") as f:\n",
    "                    f.write(json.dumps(five_q, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `questions` 는 저장될 때 `\\n` 를 기준으로 split 되어 저장됨\n",
    "    \n",
    "    - 아래처럼 태그만 추출하면 intro 설명(Here ~), 빈 값, Explanation 등 필요없는 부분을 쉽게 처리할 수 있음\n",
    "\n",
    "    - user prompt 를 ver 1 로 사용한 이유임\n",
    "\n",
    "    - `questions` 에서 `\"<q{N}>` 은 각 964개로 test 개수와 동일하게 생성됨 (간단하게 ctrl+f) \n",
    "\n",
    "    - `</q{N}>` 태그 제거 후, 질문 number 도 제거하면 쉽게 전처리 가능\n",
    "        \n",
    "        ```json\n",
    "        \"questions\": [\n",
    "            \"Here are 5 questions in Korean to help identify preventive measures from safety guideline documents:\",\n",
    "            \"\",\n",
    "            \"<q1> TSC GIRDER 조립 시 SPLICE PLATE 설치 시 작업자의 안전 거리 및 작업 공간 확보에 대한 안전 지침은 무엇인가요? \",\n",
    "            \"<q2> 철근 및 건설 자재의 안전 보관 및 이동 시 적용되는 안전 장비 및 절차는 무엇인가요?\",\n",
    "            \"<q3> 후두부 부딪힘 사고 예방을 위한 TSC GIRDER 조립 작업 시 안전 교육 및 훈련 프로그램은 무엇인가요?\",\n",
    "            \"<q4>  SPLICE PLATE 설치 작업 시 작업자의 시야 확보를 위한 안전 장치나 설계 요구 사항은 무엇인가요?\",\n",
    "            \"<q5>  작업 환경의 높은 곳에서 작업 시 안전 벨트, 안전줄 등의 안전 장비 사용 의무 및 점검 기준은 무엇인가요? \",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"\",\n",
    "            \"**Explanation:**\",\n",
    "            \"\",\n",
    "            \"* **q1:** Focuses on safe distances and workspace during SPLICE PLATE installation.\",\n",
    "            \"* **q2:**  Addresses safe handling and storage of materials to prevent accidental collisions.\",\n",
    "            \"* **q3:**  Seeks information on training programs specifically designed to prevent head injuries.\",\n",
    "            \"* **q4:**  Investigates safety devices or design requirements to ensure clear visibility during installation.\",\n",
    "            \"* **q5:**  Covers mandatory use and inspection protocols for fall protection equipment in elevated work areas.\"\n",
    "        ],\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 영어 결과가 남아있음\n",
    "\n",
    "    - 2차 전처리 필요할 듯\n",
    "\n",
    "    - 볼드체 기호 \"**\" 가 2개가 있는데 마지막인 2번째 기호에서 split 하면 됨\n",
    "\n",
    "    - 이후 남아있는 볼드체 기호를 replace 로 제거하면 될 듯\n",
    "\n",
    "        ```json\n",
    "        \"questions\": [\n",
    "            \"**E/V홀 작업 시,  자재 보관 및 안전 거리에 대한 안전 지침은 무엇인가요?**  (What are the safety guidelines for storing materials and maintaining safe distances during E/V hall work?)\",\n",
    "            \"**기계설비공사 중 이동 시,  높은 곳에서의 물체 낙하 방지 조치는 어떻게 해야 하나요?** (What measures should be taken to prevent falling objects during movement in machinery installation work?)\",\n",
    "            \"**집수정 막음 조치 후 작업자의 안전을 위한 추가적인 안전 절차는 무엇인가요?** (What additional safety procedures are required for workers after implementing water stop measures?)\",\n",
    "            \"**E/V문틀과 같은 건설 자재의 안전한 운반 및 이동 방법은 무엇인가요?** (What are the safe methods for transporting and moving construction materials like E/V frames?)\",\n",
    "            \"**작업 공간 내 물체 낙하 사고 예방을 위한 정기적인 점검 및 관리 계획은 어떻게 수립되어야 하나요?** (How should a regular inspection and maintenance plan be established to prevent falling object accidents in the work area?)\"\n",
    "        ],\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{local_path}/{temp_qe_filename}\", \"r\", encoding=\"utf-8-sig\") as f:\n",
    "    qe_data = [json.loads(line) for line in f]\n",
    "\n",
    "# 각 데이터에서 <q1>, <q2>, ...<q5>로 시작하는 질문만 추출하고 태그와 번호 제거\n",
    "# 그리고 영어 설명이 있는 경우 제거 (볼드체 기호 \"**\" 기준으로 분리)\n",
    "processed_qe_data = []\n",
    "for data in qe_data:\n",
    "    questions = data[\"questions\"]\n",
    "    filtered_questions = []\n",
    "\n",
    "    for q in questions:\n",
    "        for i in range(1, 6):\n",
    "            if q.strip().startswith(f\"<q{i}>\"):\n",
    "                # 태그와 번호 제거 처리\n",
    "                clean_q = q\n",
    "                # <q1>, <q2> 등의 태그 제거\n",
    "                clean_q = re.sub(r\"<q\\d+>\", \"\", clean_q)\n",
    "                # </q1>, </q2> 등의 닫는 태그 제거\n",
    "                clean_q = re.sub(r\"</q\\d+>\", \"\", clean_q)\n",
    "                # 숫자와 점으로 시작하는 패턴 제거 (예: \"1. \", \"2. \")\n",
    "                clean_q = re.sub(r\"^\\s*\\d+\\.\\s*\", \"\", clean_q)\n",
    "                # 앞뒤 공백 제거\n",
    "                clean_q = clean_q.strip()\n",
    "\n",
    "                # 영어 설명이 있는 경우 처리 (볼드체 기호 \"**\" 기준으로 분리)\n",
    "                if \"**\" in clean_q:\n",
    "                    # 마지막 볼드체 기호를 기준으로 분리\n",
    "                    parts = clean_q.split(\"**\")\n",
    "                    if len(parts) >= 2:\n",
    "                        clean_q = \"**\".join(parts[:-1])\n",
    "                    # 남아있는 볼드체 기호 제거\n",
    "                    clean_q = clean_q.replace(\"**\", \"\")\n",
    "\n",
    "                # 앞뒤 공백 제거\n",
    "                clean_q = clean_q.strip()\n",
    "\n",
    "                filtered_questions.append(clean_q)\n",
    "                break\n",
    "\n",
    "    if len(filtered_questions) == 5:\n",
    "        processed_qe_data.append(\n",
    "            {\"questions\": filtered_questions, \"test_id\": data[\"test_id\"]}\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"Warning: {data['test_id']}에서 5개의 질문을 찾지 못했습니다. 찾은 질문 수: {len(filtered_questions)}\"\n",
    "        )\n",
    "\n",
    "# jsonl 형식으로 전처리 결과 저장\n",
    "with open(f\"{data_path}/{qe_filename}\", \"w\", encoding=\"utf-8-sig\") as f:\n",
    "    for data in processed_qe_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dacon-hansoldeco-v3-I7BDf-XK-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
